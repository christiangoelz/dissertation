Machine Learning emerged in the 1950s as a subbranch of \gls{ai} to enable computers to learn without being explicitly programmed \cite{Samual1959}. It is defined by computational methods combining fundamental concepts from computer science, statistics, probability and optimization that automatically extract patterns and trends, i.e., \textit{learn} from data \cite{Hastie2009}. The notion of \textit{learning} therein describes the automated inference of general rules based on the observation of examples using algorithms \cite{Von_luxburg2011}. These examples should represent machine-readable data, may contain few or multiple properties, so-called features, and may be multidimensional with variable sources for example sensor recordings or pixel values.\\
The three main forms of machine learning are supervised, unsupervised, and reinforcement learning which are defined by the type of feedback a machine learning algorithm has access to during learning. In application, however, a clear distinction is often difficult, so that mixed forms such as semi-supervised learning find application \cite{Russell2015}.

\paragraph{Supervised learning}
In supervised machine learning the goal is to build a function or model that represents a generalizable relationship between input variables, known as features, and their corresponding output, referred to as labels. A learning algorithm is provided with labeled examples to optimize its performance and improve accuracy, i.e. it receives feedback on the correctness of a prediction it has made \cite{Russell2015}. The notion of generalizability is critical here and describes the ability of the learned model to predict the labels of new or \textit{unseen} data \cite{Daumé2017}. If the labels are categorical the learning task is called classification and if the labels are continuous the task at hand is regression. A standard example found in many text books is the task of email classification. Therein the input data might be emails associated with labels spam/not spam and the task is to produce a function that is able to automatically label new incoming emails \cite{Shalev2014}. For regression a standard example would be to predict the price of a house based on some features like the area or number of rooms \cite{Ng2022cs229notes}.

\paragraph{Unsupervised learning}
The goal of unsupervised learning is to find hidden structure or \textit{interesting patterns} in data without taking into account associated labels. Since there is no underlying truth defined, this type of learning is less well defined and is also referred to as \textit{knowledge discovery} \cite{Murphy2012}. Unsupervised machine learning includes many algorithms that can be further categorized according to their purpose, such as algorithms for clustering or dimensionality reduction. Clustering describes the automatically grouping of similar data points together in fewer subgroups or clusters. The goal of dimensionality reduction is to express the data in fewer properties without losing information characterizing the data \cite{Cunningham2014}. 

\paragraph{Reinforcement and other forms of learning}
Besides supervised and unsupervised, reinforcement learning is considered the third category of machine learning. The goal therein is to learn optimal actions to solve a certain problem by maximizing the reward linked to that action \cite{Murphy2012}.\\
There are also mixed forms of learning types. In Semi-supervised learning, for example, the goal is the same as for supervised learning. However the data set used to learn the model contains both, labeled and unlabeled examples and the hope is to build a stronger representation by providing more information in form of data \cite{Burkov2019}.\\
\\

\subsection{Applications to Electroencephalography}
Especially in areas where high-dimensional data is prevalent, such as in neuroscience, the use of supervised as well as unsupervised machine learning offers insight by extracting complex patterns purely data driven \cite{Bzdok2017}. With regard to the application on \gls{eeg} data it allows to address some of the main limitations of this method and automatically detect relevant patterns and their relation to a variables of interest. Various research approaches use machine learning to extract information from the highly complex data and to relate it to experimental conditions or phenomena or to detect brain states.\\
Traditionally, machine learning has been the core building block for the development of intelligent systems that can automate tasks or enhance and assist humans in performing their tasks. In the medical field the hope is to develop intelligent medical systems to inform clinical theory and support clinical decision making, i.e. assist in diagnosis, risk assessment by predicting health status or forecasting of treatment responses \cite{Woo2017}. In this context supervised learning is frequently use to identify biomarkers from \gls{eeg} by identifying signal characteristics that are predictive of a certain disease or health condition \cite{Babiloni_AlzCons2021,Mei2021}. An exciting application in the context of this work is the estimation of biological age based on regression models trained on the basis \gls{EEG} data of large population studies. Using the EEG of an individual person, the model can predict the age of the person. If the brain appears older than it would chronologically, this can be an early indication of an unfavorable state of health \cite{Engemann2022}. Beyond that, machine learning is used in developing devices with the goal to assist, augment or enhance humans capabilities such as \glspl{bci}.  In \glspl{bci}, neural activity is decoded, using classification to generate control commands for various external devices such as computers or prosthetic limbs \cite{Saha2021, Anumanchipalli2019}.
\begin{itemize}
next BCI 
\item{Scientific Context: Statistical learning als alternativ ansatz}
\item{Decoding Ansätze} 
\end{itemize}





. This is not only relevant for solving practical problems in biomedical engineering, but also gaining insights and drawing conclusions by testing the generalizability of hypotheses and developing new theories \cite{Glaser2019, Hastie2009}. 
\\
Dimensionality reduction, for example, makes it possible to describe the structure of high-dimensional data with fewer properties \cite{Cunningham2014}. Two prevalent methods in the analysis of neuroscientific data are \gls{pca} and \gls{ica}, both of which are used to uncover meaningful patterns in the data. \Gls{ica}, in particular, has gained popularity for its ability to reveal brain networks and statistical sources in \gls{eeg}, enabling inferences to be made about both noise components and brain processes during behavior \cite{Strophal2018}. In addition, \gls{dmd}, as applied  by \citeauthor{Brunton2016} \cite{Brunton2016}, allows to map both the spatial and temporal structure of sleep spindle networks as well as task related brain networks involved in movements. Furthermore, non-linear methods like \gls{umap} offer to visualize the structure of high-dimensional EEG signals \cite{Banville2021}.\\
\\
Besides unsupervised, supervised approaches are being frequently used to map brain function to external variables, such as group membership, behavior or stimuli, with the ultimate goal to identify predictive variables or features \cite{Glaser2019}.
This is often of interest to inform clinical theory in the development of novel biomarkers that enable the automatic diagnosis and characterization of age-related pathologies such as Alzheimer's or Parkinson's disease \cite{Babiloni_AlzCons2021,Mei2021}. In basic neuroscience the goal is to use classification and regression to draw inferences about underlying mechanisms of the phenomenon under study. In this context regression is used to predict the neural response to an external variable, such as a particular stimulus, to create a so-called encoding model, which can be used to test and compare brain computational theories \cite{Kriegskorte2019, Naselaris2011}. \citeauthor{Benjamin2018} \cite{Benjamin2018}, for example, show the superiority of using modern machine learning in predicting spike rates based on behavioral measurements. Contrary, decoding models are created with the goal to predict external variables from neural activity providing insights in the information content of neural measurements. This type of analysis is often referred to as \gls{mvpa} because its goal is to detect multivariate patterns, e.g., a set of voxels in \gls{fMRI} or an electrical pattern at a given time in \gls{eeg}, associated with an experimental condition based on goodness of classification or regression \cite{Holdgraf2017}. For example, neural dynamics in visual object perception \cite{Cauchoix2014} or working memory \cite{Bae2018} have been studied using decoding approaches applied to \gls{eeg} recordings.\\


Beyond that, machine learning is used in developing devices with the goal to assist, augment or enhance humans capabilities such as \glspl{bci}. In \glspl{bci}, neural activity is decoded, using classification to generate control commands for various external devices such as computers or prosthetic limbs \cite{Saha2021, Anumanchipalli2019}.







intelligent medical systems to support clinical decision making, i.e. assist in diagnosis, risk assessment by predicting health status or forecasting of treatment responses \cite{Woo2017}.




% Furthermore, a focus on biological age instead of chronological age promises the identification of atypical aging trajectories and the prediction of future health outcomes. The concept of \textit{brain age} describes the biological age of the brain \cite{Franke2019}. This is based on large population studies, which can be used to train regression models that model the relationship between chronological age and a measured variable, e.g. MRI scans or EEG data \cite{Engemann2022, Dadi2021}. The biological age of a person's brain can then then be estimated by the prediction of the model given the persons brain data and compared to the population distribution of brain data. An older appearing brain was thereby reported in connection with an unfavorable state of health \cite{Gonneaud2021}.\\










% Examples of automation can be found, among others, in relation to the processing of \gls{eeg} data addressing low reproducibility and subjectivity. \citeauthor{Jas2017} \cite{Jas2017} use unsupervised machine learning to automate data prepossessing, i.e. rejecting and interpolating bad data segments. In addition, machine learning is increasingly used for electrical source imaging to solve the inverse problem, i.e. finding neural sources that give rise to the recorded \gls{eeg} activity \cite{Cui2019,Hecker2021}.\\





% \subsection{Learning}
% The process of learning can be formally defined using statistical learning theory \cite{Von_luxburg2011,Shalev2014}. In this mathematical framework the learning task, be it classification, regression, dimensionality reduction or clustering share a couple of fundamental building blocks. These are an arbitrary domain of examples whi


% space of examples, $\mathcal{Z}$, the space of potential solutions, $\mathcal{H}$,  also called hypothesis space and a \textit{loss function}, $\ell$, which measures the "cost" of a false prediction or an inaccurate representation of the data in supervised and unsupervised learning respectively. \\
% In classification for example 


% \Citeauthor{Russell2015} \cite{Russell2015} define the process and with that the basic formal principle of supervised machine learning algorithms as follows. Learning typically is based on a training set $D=\{(x_i, y_i)\}_{i=1}^{m}$ containing $m$ examples $(x_i , y_i)$ in which $x_i$ is a feature vector and $y_i$ the associated label for the \({i}^{th}\) example. It is assumed that $y_i$ was generated by some unknown function $y=f(x)$ and the goal of learning is to discover a function $h$ from a given hypothesis space $\mathcal{H}$ that approximates the true function $f$. Note that $f$ does not necessarily represent a function but can also be stochastic in nature, so that the goal is to learn a probability distribution, i.e. $P(Y,x)$. In this process a loss function $L(y,\hat{y})$ is defined, which determines the error between the actual label, $y$ and the models prediction $\hat{y}$. Thus, the goal of a learning algorithm can be defined as minimizing $L$. As the underlying distribution from which $D$ was drawn is unknown the empirical loss is typically defined as: 
% \begin{equation}
% EmpLoss_{L,D}(h)=\frac{1}{m}\sum_{(x,y)\in{D}}L(y,h(x)) .
% \end{equation}
% Following the best hypothesis $\hat{h}^*$ is chosen based on: 
% \begin{equation}
% \hat{h}^*=\operatorname{argmin}_{h\in{\mathcal{H}}}EmpLoss_{L,D}(h) .
% \end{equation}
% In practice, this often involves the automated selection of those parameters $\theta$ of a chosen class of models ($\mathcal{H}$) that minimize $EmpLoss$. This optimization problem can then be solved by various algorithms such as gradient descent, stochastic gradient descent and sequential minimal optimization. The choice depends on the problem to be solved as well as the selected model. 


  

% \begin{figure*}[h]
%   \dummyfig{Categories of ML} 
%   \caption{Categories of ML}
%   \label{fig1:ml_types}
% \end{figure*}

% % \subsection{A formal description of learning}
% % A formal definition of how a machine learns, can be made based on statistical learning theory. \citeauthor{Von_luxburg2011}\cite{Von_luxburg2011} describe this framework for supervised learning as follows. Given a learning problem, \(\mathcal{X}\) denotes the space of inputs and \(\mathcal{Y}\) the space outputs. The goal of learning is then to learn a functional relationship $f:\mathcal{X}\rightarrow\mathcal{Y}$. 
% % Further, $\mathcal{P}$ represents a random probability distribution over $\mathcal{Z}=\mathcal{X}\times\mathcal{Y}$ from which a training data set $S$ is drawn at random. Using $S$ the goal of supervised machine learning is to learn a function $h:\mathcal{X}\rightarrow\mathcal{Y}$. The performance of this function can be measured using a so called loss function $ell$ measuring the error a prediction 
% % The learning process typically involves adjusting the parameters defining the model by presenting the input-output pairings in the training data set to minimize the error, called loss, between the actual value and the models prediction. This is done by optimizing a cost function, defined as the summarized loss over the training examples. Often used optimization algorithms are gradient descent, stochastic gradient descent and sequential minimal optimization\footnote{Since the focus of this work is rather on the application and the evaluation with regard to scientific knowledge gain, a detailed mathematical introduction of the different algorithms is omitted at this point (see \cite{Ng2022cs229notes} for a detailed description).} and the choice depends on the problem to be solved as well as the selected model. It should be noted that not all learning algorithms are based on minimizing a defined cost function, but that there are also algorithms that implicitly optimize a certain criterion, such as Decision Trees or the K-Neirest Neighbor algorithm.\\
% % More formally, supervised learning is to learn a function, also called hypothesis, mapping the space of input values \(\mathcal{X}\) to the space of output values \(\mathcal{Y}\), i.e. \(h_{\theta}:\mathcal{X}\rightarrow\mathcal{Y}\). by minimizing a cost function \(J(\theta)\). 
% % Here \(\theta\) are the parameters, or weights, defining the space of functions and the cost function can be defined as 
% % $$J(\theta) = \frac{1}{m} \sum_{i=1}^{m} L(h_{\theta}(x_i),y_i)$$.  
% % Here \(L\) is called loss function which quantifies the error between true label \(y_i\) and predicted label 

% % A standard example found in many text books is the task of email classification. Therein the input data might be emails associated with labels spam/not spam and the task is to produce a function that is able to automatically label new incoming emails \cite{Shalev2014}. For regression a standard example would be to predict the price of a house based on some features like area, number of rooms, etc. \cite{Ng2022cs229notes}.
% % Inductive reasoning is central to this process and involves using labelled input-output pairs known as training data set $S=\{(x_i, y_i)\}_{i=1}^{m}$, in which $x_i \in \mathbb R^{n}$ is a ${n}$-dimensional feature vector and $y_i$ the associated label for the \({i}^{th}\) example generated by some unknown function $y_i=f(x_i)$ \cite{Russell2015, Saha2021}. An algorithm is then used to induce a function $h$ approximating the true function $f$ \cite{Daumé2017}. Note that $f$ does not necessarily represent a function but can also be stochastic in nature, so that the goal is to learn a probability distribution, i.e. $P(Y,x)$ \cite{Russell2015}. 


% \subsection{Applications in neuroscience}
% Both fields, research on \gls{ai} and neuroscience, are strongly interconnected as information processing in the brain serves as role model for the ultimate goal in \gls{ai} research, creating an artificial general intelligence system that matches or even surpass human intelligence \cite{Macpherson2021}. This led, for example, to the development of artificial neural networks, a class of machine learning algorithms that underlie modern advances in the field of \gls{ai} \cite{Cox2014}. Other examples are so called \glspl{cnn}, which are used in computer vision and inspired by the architecture of the visual ventral stream or \glspl{rnn} mimicking the functioning of working memory \cite{Macpherson2021, Fukushima1982, Yin2020}. On the other side, \glspl{cnn} and \glspl{rnn} are used to understand functional and organizational properties of the visual system \cite{Yamins2014} or working memory \cite{Kim2021} which highlights the bilateral relationship between machine learning and neuroscience and so its use in neuroscience is widespread. The goals incorporate not only solving practical problems in biomedical engineering or data processing, but also gaining insights and drawing conclusions by testing hypotheses and developing new theories.\\
% Traditionally, machine learning has been the core building block for the development of intelligent systems that can automate tasks or enhance and assist humans in performing their tasks. Examples of automation in the field of neuroscience can be found, among others, in relation to the processing of neuroscientific data addressing low reproducibility and subjectivity. \citeauthor{Jas2017} \cite{Jas2017} use unsupervised machine learning to automate data prepossessing, i.e. rejecting and interpolating bad data segments, in the analysis of data from \gls{eeg} or \gls{meg}. In addition, machine learning is increasingly used for electrical source imaging to solve the inverse problem, i.e. finding neural sources that give rise to the recorded \gls{eeg} activity \cite{Cui2019,Hecker2021}. Other applications involve classification to automatically segment images from \gls{mri}, e.g. in grey or white matter, or the automatic detection and quantification of cell activation in calcium imaging \cite{Akkus2017}. In medical settings furthermore machine learning can be applied with the goal to support clinical decision making, i.e. assist in diagnosis, risk assessment by predicting health status or forecasting of treatment responses \cite{Woo2017}. Examples include classification to support the diagnosis of Alzheimer's disease based on EEG \cite{Gallego-Jutglà2015} or \gls{mri} \cite{Vemuri2008} data as well as the automated identification of lesions in radiographic brain images \cite{Zhou208} or the detection of epileptic seizures based on EEG data \cite{Vandecasteele2020}. Beyond that, machine learning is often used in developing devices with the goal to assist, augment or enhance humans capabilities such as \glspl{bci}. In \glspl{bci}, neural activity is decoded, using classification to generate control commands for various external devices such as computers or prosthetic limbs \cite{Saha2021, Anumanchipalli2019}. In this context unsupervised methods, i.e. dimensionality reduction, often are used to deal with the high dimensionality and variability of measured neural activity \cite{Brunton2019}.\\
% While the examples described so far have focused on solving specific problems, machine learning algorithms provide mechanistic insights into the information structure of data and the generalizability of hypotheses \cite{Hastie2009}. Especially in areas where high-dimensional data is prevalent, such as in neuroscience, the use of supervised as well as unsupervised machine learning offers insight by extracting complex patterns purely data driven \cite{Bzdok2017}. Dimensionality reduction, for example, makes it possible to describe the structure of high-dimensional data with fewer properties \cite{Cunningham2014}. Two common methods in the analysis of neuroscientific data are \gls{pca} or \gls{ica} which are used to uncover meaningful patterns in the data. Especially \gls{ica} is increasingly applied to uncover brain networks based on \gls{fmri} data \cite{Varoquaux2010} or to uncover the statistical sources in \gls{eeg} which can be used to infer components of noise but as well on brain processes during behavior \cite{Strophal2018}.  In addition, with \gls{dmd}, \citeauthor{Brunton2016} \cite{Brunton2016} apply a method to \gls{ecog} that allows to map both the spatial and temporal structure of sleep spindle networks as well as task related brain networks involved in movements. Besides dimensionality reduction, clustering is used to identify subgroups at different levels ranging from molecules to participant groups giving rise to new testable hypothesis \cite{Vu1601}. By applying a nonlinear dimensionality reduction technique called \gls{umap} and clustering to data of extracellular waveforms recorded in the premotor cortex of macaques, \citeauthor{Lee2021} \cite{Lee2021} were able to identify subgroups of neurons with previously unknown diversity in terms of dynamics and laminar distribution. Furthermore, in another study analysing \gls{fmri} during different cognitive tasks, \citeauthor{Hawco2021} \cite{Hawco2021} assessed the variability structure of brain activation patterns and identified a spectrum of participants brain activation patterns that coincides with task performance. These results were only possible using advanced methods from machine learning. Besides unsupervised approaches, supervised approaches are being used to map brain function to external variables, such as group membership, behavior or stimuli, with the ultimate goal to identify predictive variables or features \cite{Glaser2019}. Under this objective classification of group membership, inferences can be made about the underlying mechanisms of the phenomenon under study. This is often of interest to inform clinical theory in the development of novel biomarkers, for example in autism \cite{Deshpande2013} or Alzheimer's disease \cite{Farina2020}, as well as in basic neuroscience. In this context, regression is used to predict the neural response to an external variable, such as a particular stimulus, to create a so-called encoding model, which can be used to test and compare brain computational theories \cite{Kriegskorte2019, Naselaris2011}. \citeauthor{Benjamin2018} \cite{Benjamin2018}, for example, show the superiority of using modern machine learning in predicting spike rates based on behavioral measurements. Contrary, decoding models are created with the goal to predict external variables from neural activity providing insights in the information content of neural measurements. This type of analysis is often referred to as \gls{mvpa} because its goal is to detect multivariate patterns, e.g., a set of voxels in fMRI or an electrical pattern at a given time in EEG, associated with an experimental condition based on goodness of classification or regression \cite{Holdgraf2017}. For example, neural dynamics in visual object perception \cite{Cauchoix2014} or working memory \cite{Bae2018} have been studied using decoding approaches applied to \gls{eeg} recordings. Decoding was also linked to another variable, such as group membership \cite{Csizmadia2021, Bae2020} or symptom scores \cite{Coutanche2011} , to reveal interesting interrelationships with neural representations of the condition under study.\\
% In summary, the application of machine learning in neuroscience is diverse. In addition to a close theoretical integration of the two fields, machine learning offers solutions to engineering problems in supporting and augmenting human performance as well as in data processing. In basic sciences, it is used to extract patterns from highly complex data and gain insights into the predictive power of variables. This can contribute to the verification and formation of new hypotheses.


% \subsection{Methodological considerations}
% Methods from supervised and unsupervised machine learning are possible candidates. In unsupervised machine learning, the goal is to find structure in the data. This includes methods for dimensionality reduction and clustering. Dimensionality reduction for example allows us to describe the structure of high dimensional data in fewer properties [20]. Two common methods in the analysis of neurophysiological data are the principal component analysis (PCA) and independent component analysis (ICA). These allow the detection of spatial patterns in the data that represent the underlying network characteristics of neurophysiological data [8]. In addition, with dynamic mode decomposition (DMD), Brunton, Johnson, et al. [21] apply for the first time a method to electrophysiological data that allows us to map both the spatial and temporal structure of the network structure of neurophysiological data.
% In supervised machine learning, models are created that can predict a certain outcome based on input data. This method is used to detect neuronal representations of the environment or certain behaviors as well as group memberships and to identify relevant markers [22].
% In the context of lifespan changes, complex brain network behavior based on EEG data could be extracted and visualized using dimensionality reduction. Supervised machine learning methods could be used to detect representations of the environment and behavior and to draw conclusions about the differentiation of brain networks. Automatic detection of group membership could further provide new predictors of nervous system states.

