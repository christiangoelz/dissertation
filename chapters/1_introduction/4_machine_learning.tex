Machine learning emerged in the 1950s to enable computers to learn without being explicitly programmed \cite{Samual1959}. It is defined by computational methods combining fundamental concepts from computer science, statistics, probability and optimization that automatically extract patterns and trends, i.e., \textit{learn} from data \cite{Hastie2009}. The notion of \textit{learning} therein describes the automated inference of general rules based on the observation of examples using algorithms with the goal to solve a certain task or problem \cite{Von_luxburg2011}. Often, in its basic form these tasks involve making predictions based on learned relationships or the extraction of information based on automatically detected patterns and structures from data. Many problems can be formulated by these tasks and a rise in machine learning started in the 1990s to 2000s with the availability of computing resources, data, and the development of algorithms, which have found their way into everyday life not only since the current hype about generative \gls{ai}systems. Examples can be found in numerous areas, such as predicting stock prices, personalized advertising, or autonomous driving \cite{Rudin2014}.\\
In science machine learning is increasingly used as a complementary method to classical statistical analyses because of the ability to make predictions as well as to deal with the multidimensional structure and non linearity in real world data sets for drawing inference \cite{Bzdok2018}. Especially in areas where high-dimensional data is prevalent, such as in neuroscience, the use of methods from machine learning offers insight by extracting complex patterns purely data driven \cite{Brunton2019}. In terms of \gls{eeg} this means that machine learning can help identify subtle patterns and nonlinear relationships from the multidimensional complex structure of \gls{eeg} data, allowing for more accurate and efficient analysis of brain recordings. A wide variety of methods are available for this purpose, which can be roughly characterized on the basis of various properties. 

\subsection{Forms of machine learning}
The three main forms of machine learning are supervised, unsupervised, and reinforcement learning which are defined by the type of feedback a machine learning algorithm has access to during learning \cite{Shalev2014}.\\
In supervised machine learning the goal is to learn a generalizable relationship between data and associated information, so-called labels or target. This can then be used to predict the label of new data that not have been used during the process of learning. If the labels are categorical, the prediction task is called classification; for continuous labels, the term is regression. The goal of unsupervised machine learning is to find hidden structure in data without taking into account associated labels. This could be grouping similar data points, i.e. clustering, or uncovering a meaningful low dimensional representation of high dimensional data, i.e. dimensionality reduction. This type of learning is also referred to as \textit{knowledge discovery}\cite{Murphy2012}. Reinforcement learning describes the task to learn optimal actions to solve a certain problem by maximizing the reward linked to that action. See \autoref{fig:ml_forms} for an overview.

\begin{figure}[h]
\begin{center}
\begin{tikzpicture}[grow cyclic,
	level 2/.append style={level distance=2cm},]
\node[draw,ellipse,align=center,font=\bfseries\large]{Machine\\learning}
child[grow=145,level distance=4cm] { node[draw,ellipse,align=center] {Reinforcement\\learning}
}
child[grow=35, level distance=4cm] {  node[draw,ellipse,align=center] {Supervised\\learning}
	child[grow=-30, level distance=3.8cm] { node {Classification}}
	child[grow=-90]{ node {Regression}}
}
child[grow=-90, level distance=2.5cm] {node[draw,ellipse,align=center] {Unsupervised\\learning}
	child[grow=south west,level distance=2.5cm] { node[align=center] {Dimensionality\\reduction}}
	child[grow=south east] { node {Clustering}}
}
node at (-3.5,-0.5)[scale=0.9]{\input{figures/reinforcement.pdf_tex}}
node at (1.75,-6)[scale=0.25]{\input{figures/clustering.pdf_tex}}
node at (-1.6,-6)[scale=0.25]{\input{figures/dim_red.pdf_tex}}
node at (3.5,-1)[scale=0.25]{\input{figures/regression.pdf_tex}}
node at (6.5,-1.125)[scale=0.25]{\input{figures/classification.pdf_tex}}
;
\end{tikzpicture}
\end{center}
\caption[The three main forms of machine learning]{The three main forms of machine learning}
\label{fig:ml_forms}
\end{figure}

\noindent In practice however a clear separation is often not possible. As such, dimensionality reduction can also be supervised, i.e. labels are provided to learn a new representation of the data \cite{mcinnes2018umap}. Besides, in semi-supervised learning, for example, the goal is the same as for supervised learning. However the data set used to learn the relationship contains both, labeled and unlabeled examples and the hope is to build a stronger representation by providing more information in form of data \cite{Burkov2019}. \\
In addition traditional machine learning is often contrasted to deep learning methods involving the use of artificial neural networks which are composed of many layers of interconnected nodes often used in an end to end fashion in which features are extracted within the network. Usually they require huge amount of data and computational power. In the context of this thesis the tasks considered involve the processing of a \gls{eeg} from experiments with mid to small sample sizes with the goal to learn meaningful patterns and relationships in data. The most commonly used forms in this contexts are supervised and unsupervised learning focusing traditional machine learning. For this reason these forms will be the focus in the following chapters.

\begin{tcolorbox}[breakable, enhanced]
    \subsection*{Excursus: How does a machine learn?}
    "A computer program is said to learn from experience $E$ with respect to some task $T$ and some performance measure $P$, if its performance on $T$, as measured by $P$, improves with experience $E$" \cite{Mitchell1997}. In other words learning in the context of machine learning typically involves solving a specific task by using algorithms that improve their performance by using example data. There are numerous algorithms designed to solve the problems outlined above. Thereby some basic building blocks can be defined which can be used to describe computational learning in a formal way. In the following description the view of statistical learning theory is considered, notation is adapted from \citeauthor{Shalev2014}\cite{Shalev2014}, \citeauthor{Von_luxburg2011}\cite{Von_luxburg2011}.\\
    \\
    Learning always is based on data, i.e. measurable information of some phenomenon, which consists of attributes of the phenomenon, so called features, and in supervised learning an associated label. It can mathematically defined as open bounded set $\mathcal{Z}\subset\mathbb{R}^n$ of dimension $n$. Typically there is only a set of examples or training data $S=\{z_i,...,z_m\}\subset{\mathcal{Z}}^m$ available, where $i = 1,\dots,m$, and each $z_i$ is sampled independently from $\mathcal{Z}$ according to an underlying probability distribution $\mathcal{D}$. Thus the only assumption made is that the example data are independent and identically distributed. No assumption on $D$ is made.\\
    In supervised learning $\mathcal{Z}$ comprises the space of input data $\mathcal{X}$ and the space of labels or output $\mathcal{Y}$. The example data $S$ consists of labeled input-output pairs $z_i=x_i,y_i\in(\mathcal{X}\times\mathcal{Y})^m$, where $x_i$ is an input data vector and $y_i$ is its corresponding output label. The pairs are sampled by some unknown joint probability distribution $\mathcal{D}$ on the space $\mathcal{X}\times\mathcal{Y}$.\\
    The space $\mathcal{Z}$ in unsupervised learning comprises the input data space $\mathcal{X}$ only and the example set $S$ consists of unlabelled examples $z_i=x_i\in\mathcal{X}^m$, sampled according to some unknown probability distribution $\mathcal{D}$ on the space $\mathcal{X}$.\\
    Learning ultimately can be thought of as approximating an underlying ground truth function $f$, also called model, that represents the relationship between input and output in supervised learning, i.e., 
    \begin{equation}
    f:\mathcal{X}\rightarrow\mathcal{Y},
    \end{equation}
    or the mapping to a space of hidden patterns or structure $\mathcal{W}\subset\mathbb{R}^p$, where $p$ can be equal or smaller than $n$, i.e,
    \begin{equation}
    f:\mathcal{X}\rightarrow\mathcal{W}.
    \end{equation}
    A learning task can be conceptualized as searching through the space of all possible solution functions. As this is not feasible a finite class of functions, so called hypotheses, is typically selected a priory. Thus, learning can be thought of as selecting a hypothesis $h$ from a space of potential solutions $\mathcal{H}$ with $\mathcal{H}=\{h:\mathcal{X}\rightarrow\mathcal{Y}\}$ in supervised learning and $\mathcal{H}=\{h:\mathcal{X}\rightarrow\mathcal{W}\}$ in unsupervised learning. \\
    A learner or learning algorithm is the means of selecting the best element from $\mathcal{H}$.
    The cost of a false prediction or an inaccurate representation of the data is quantified using a loss function, $\ell:\mathcal{H}\times\mathcal{Z}\rightarrow\mathbb{R}_+$. In other words it measures how well a specific hypothesis is doing.\\
    The expected risk is measure of the average loss of a hypothesis, $h\in\mathcal{H}$ with respect to the probability distribution $\mathcal{D}$ over $\mathcal{Z}$ and can be defined as
    \begin{equation}
    L_{D}(h):=\mathbb{E}_{z\sim D}[\ell(h,z)]
    \end{equation}
    A learner should select a hypothesis with lowest possible expected risk. However, the underlying probability distribution is unknown. By using $S$ the expected risk can be estimated by using the empirical risk over the training data. This is defined by:
    \begin{equation}
    L_{S}(h):=\frac{1}{m}\sum_{i=1}^m\ell(h,z_i).
    \end{equation}
    Following this, learning can be formalized solving an optimization problem of the form: 
    \begin{equation}
    \hat{h}=\arg\min_{h\in\mathcal{H}}L_{S}(h),
    \end{equation}
    which can be solved computationally. In parameterized models, this often involves the automated selection of those parameters $\theta\in\Theta$ of a chosen class of models that minimize $L_{S}(h_\theta)$. This optimization problem can then be solved by various methods such as gradient descent, or e.g., analytically using least squares estimation. The solution $\hat{h}$ is the learned model that can be used to solve the task at hand, e.g. predicting the label of new input data or uncovering patterns or structure in data. This is known as \gls{erm}.\\
    Upon \gls{erm} more complex learning paradigms can be used addressing common problems such as overfitting, in which the learned hypothesis to closely relies on the training data and therefore has low generalization performance, e.g. regularized risk minimization which introduces regularization to \gls{erm} or structural risk minimization that penalizes complex models and encourages simplicity.\\
    \\
    Although most machine learning can conceptualized within the framework of \gls{erm} there are models that instead of minimizing risk assume that the underlying distribution over the data has a specific parametric form and the goal is to estimate these parameters by using \gls{mle} which seeks to find the model parameters that maximize the likelihood of the observed data under the assumed parametric distribution, i.e. \\
    \begin{equation}
    \hat{\theta}_{\text{MLE}} = \arg\max_{\theta\in\Theta}\prod_{i=1}^{m}p_{\theta}(z_i),
    \end{equation}
    where $p_{\theta}(z)$ is the joint probability function of the assumed parametric distribution and $\hat{\theta}_{\text{MLE}}$ is the estimated value of the parameter vector $\theta$.
\end{tcolorbox}

\subsection{Applications in the context of aging research}
Traditionally, machine learning has been the core building block for the development of intelligent systems that can automate tasks or enhance and assist humans in performing their tasks. Such systems are not only important in terms of assistive technology, for example to support elderly people with disabilities to live their daily lives, but are also relevant in the medical fields. In the latter the hope is to develop intelligent medical systems to inform clinical theory and support clinical decision making, i.e. assist in diagnosis, and risk management by predicting health status or forecasting of treatment responses \cite{Woo2017}. In this context, supervised learning is often used to identify markers from \gls{eeg} by identifying signal features that are predictive of a particular disease or health condition, which is highly important in terms of promoting a healthy aging trajectory \cite{Babiloni_AlzCons2021,Mei2021}. An application is the estimation of biological age based on regression models trained on the basis on neural data, e.g. \gls{eeg} data, recorded in large population studies \cite{Engemann2022}. Using data of an individual person, a regression model can predict the age of that person. If the brain appears older than it would chronologically, this can be an early indication of an unfavorable state of health \cite{Gonneaud2021}.\\
Another in the context of aging highly relevant application is the development of devices with the goal to assist, augment or enhance humans capabilities such as \glspl{bci}. In \glspl{bci}, neural activity is decoded, using classification to generate control commands for various external devices such as computers or prosthetic limbs \cite{Saha2021, Anumanchipalli2019}. Decoding hereby refers to learning a classification or regression model that is capable of predicting behavioral outcomes or cognitive states based on neural data.\\
\\
Beyond the application in \glspl{bci}, decoding techniques are widely used in neuroscientific research to gain insights into the neural mechanisms underlying perception, cognition, and behavior. This type of analysis is often referred to as \gls{mvpa} because its goal is to detect multivariate patterns, e.g., a set of voxels in \gls{fmri} or an electrical pattern at a given time point in \gls{eeg}, associated with an experimental condition \cite{Holdgraf2017}. While the use has a long history in the field of \gls{fmri} analysis, it has only become more widespread in the field of \gls{eeg} in recent years.  Therefore, decoding approaches to understand age-related reorganization are mostly limited to fMRI studies. A common approach is to measure dedifferentiation, i.e. the loss of neural specificity, at the individual level. Since dedifferentiation by definition results in more similar brain activation patterns for different tasks or stimuli, poorer performance of classifiers trained to discriminate between them based on neural recordings is indicative of a less distinctive neural representation \cite{Koen2019, Park2010}. \\ 
Furthermore, the classification of group membership or group level regression can provide information about interesting relationships and their generalizability. Particularly for \gls{eeg} markers representing functional network characteristics can reveal insightful findings about the relationship to age-related changes \cite{Petti2016}.\\
In addition to the application of supervised learning algorithms, unsupervised learning algorithms have long been used in neuroscience. Unsupervised methods are often used as a preprocessing step to reduce the complexity of \gls{eeg} data but can also provide interesting insights into the structure of data sets. Dimensionality reduction algorithms can provide insights into high-dimensional data by providing a possibility to visualize high dimensional patterns of \gls{eeg} data \cite{Kottlarz2020, Banville2021}. This can be used to study the temporal structure of \gls{eeg} signals in terms of the dynamic of brain networks with aging \cite{Brunton2016, vieluf2018age}.\\
\\
In summary, the use of machine learning is very diverse and ranges from engineering applications to scientific knowledge discovery. Especially in the latter case, it offers the advantage of automated extraction of patterns from highly complex data that can contribute to the study of age-related changes. While decoding approaches are particularly interesting for measuring age-related changes in the organization of neural systems, such as the level of differentiation, group analysis could provide new insights into datasets. Especially classification methods that allow to predict a certain experimental condition or a group membership are particularly suitable. The combination with unsupervised learning algorithms such as dimensionality reduction methods could be particularly beneficial and used for the visualization of high-dimensional data. Based on this, the next section will present common approaches with a focus on the classification and dimensionality reduction of EEG data.

\subsubsection{State of the art approaches}
Selecting a suitable learning algorithm for classification, i.e. classifier, involves an iterative approach that divides the available data into training and testing sets, trains and validates various classifiers within the training portion (cross-validation), and finally tests the best performing model on the testing portion to estimate its ability to generalize \cite{Hastie2009}. Different classifiers may have varying strengths and weaknesses, with popular examples including \glspl{svm}, \gls{lda}, and \glspl{rf} (see \cite{shoorangiz2021eeg} for an overview).\\
While deep neural networks are capable of learning on the basis of raw data their advantage comes into play with large labeled data resources, which are often expensive to acquire in the case of \gls{eeg} \cite{Banville2021}. Traditional learning approaches can be more efficient with good performance and promise better interpretability, especially for comparatively smaller data sets and limited computational resources \cite{Gemein2020}. Due to the low signal-to-noise ratio and high complexity of \gls{eeg} data, the inputs in these approaches are typically represented by well known \gls{eeg} characteristics, or features, that are believed to be related to the problem being learned. Typical features include parameters of time, frequency, time-frequency, connectivity and information theoretical parameters extracted for each sensor (see \cite{Gemein2020} for common choices). However, this approach may lead to less flexible and generalizable models with low spatial resolution and vulnerability to low signal-to-noise ratios \cite{Saeidi2021}.\\
To address this, some approaches compute the sources of the \gls{eeg} signals in the brain using biophysical models as a preprocessing step prior to feature extraction \cite{Khan2018, Westner2018}. Other approaches use supervised and unsupervised decomposition techniques belonging to the field of dimensionality reduction as a preprocessing step for further prediction tasks or provide information themselves in the sense of knowledge discovery. These methods aim at \textit{unmixing} the highly correlated sensor time series by assumptions about the underlying signal components. For example, \gls{ica} assumes statistical independence, while \gls{pca} assumes that the extracted components are maximally uncorrelated to each other, capturing the largest amount of variance in the data \cite{CohenX2017}. \Gls{dmd} is a method that explicitly considers the temporal structure of the signals, which requires the extracted signal patterns (modes) to be temporally coherent, thus accounting for the network character of the brain \cite{Brunton2016}. Additionally, supervised methods such as \gls{csp} \cite{Blankertz2008} or xDAWN \cite{rivet2009xdawn} extract signal components that correlate with the labels to be predicted.\\
While supervised and unsupervised ways of examining the complex \gls{eeg} signals in terms of components and patterns to generate knowledge, non-linear methods such as \gls{tsne} and \gls{umap} take into account the non-linear relationships between the data points and provide a lower-dimensional representation of the data that is often easier to interpret and visualize \cite{mcinnes2018umap}. These methods can be particularly useful for exploring the relationships between different \gls{eeg} features or for identifying subgroups within a dataset.\\
It is important to note that these methods can be applied not only to the \gls{eeg} signals itself, but also to previously extracted \gls{eeg} parameters or in combination in terms of knowledge discovery. Thus, supervised and unsupervised dimension reduction provides data-driven insights into the complex underlying information, but also serves as preprocessing for further tasks such as prediction.

