Machine Learning emerged in the 1950s as a subbranch of \gls{ai} to enable computers to learn without being explicitly programmed \cite{Samual1959}. It is defined by methods that automatically extract patterns and trends, i.e., \textit{learn} from data \cite{Hastie2009}. The notion of \textit{learning} therein describes the process of automatically building a mathematical or computational representation, i.e. a model, based on example data that can then be used to predict future data or to solve any other kinds of practical problems, such as decision making or dealing with high dimensional data \cite{Murphy2012, Burkov2019}. Besides \textit{learning}, the term \textit{training} is commonly used to describe this process\footnote{Both terms will be used interchangeable throughout this manuscript}. The data used for learning may contain few or multiple properties, so-called features, and may be multidimensional with variable sources for example sensor recordings or pixel values.

\subsection{Types of machine learning}
\label{theory:ML:categories}
Machine learning can be subdivided into three categories, supervised, unsupervised and reinforcement learning \cite{Burkov2019, Murphy2012} (see Figure \ref{fig1:ml_types}. The categories are defined by the way in which learning occurs and the goal or problem to be solved. The following definitions are based on \cite{Murphy2012,Burkov2019}.\\
\begin{figure*}[h]
  \dummyfig{Categories of ML} 
  \caption{Categories of ML}
  \label{fig1:ml_types}
\end{figure*}

\subsubsection{Supervised learning}
In supervised learning, the goal is to produce a model representing the mapping between input data \(x\) and associated information or descriptions, called labels or targets, \(y\). The learning takes place on the basis of labelled input-output pairs which are combined in a so-called training data set \(\mathcal{D}=\{(x_i, y_i)\}_{i=1}^{m}\), in which \(x_i \in \mathbb R^{n}\) is a \({n}\)-dimensional feature vector and \(y_i\) the associated label for the \({i}^{th}\) example. The model then can be used to predict the label of new data that not have been used during training. If the labels are categorical, i.e. \(y_i \in \{1,2...C\}\) it is called classification; for continuous labels, i.e. \(y_i \in \mathbb R\), it is called regression. Learning involves adjusting parameters defining the model by presenting the input-output pairings in the test data set to minimize the error between the actual value and the models prediction, which is done by optimizing a loss function defining this error. Typical optimization algorithms involve, gradient descent, stochastic gradient descent and sequential minimal optimization\footnote{Since the focus of this work is rather on the application and the evaluation with regard to scientific knowledge gain, a detailed mathematical introduction of the different algorithms is omitted at this point (see \cite{Ng2022cs229notes} for a detailed description).} and the choice depends on the problem to be solved and the selected model. Model categories involve generative algorithms, such as Gaussian mixture models or Naive Bayes', and deterministic models, such as support vector machines and logistic regression. 

\subsubsection{Unsupervised learning}
The goal of unsupervised machine learning is to find hidden structure in data without taking into account associated labels, i.e. \(\mathcal{D}={(x_i)}_{i=1}^{N}\). This could be grouping similar data points, i.e. clustering, or uncovering a meaningful low dimensional representation of the data, i.e. dimensionality reduction.\\

\susubsection{Reinforcement learning}
Rather than a specific label in reinforcement learning the goal is to learn optimal actions to solve a certain problem by maximizing the reward linked to that action.\\



\subsection{Applications in neuroscience}
Both fields, research on \gls{ai} and neuroscience, are strongly interconnected as information processing in the brain serves as role model for the ultimate goal in \gls{ai} research, creating an artificial general intelligence system that matches or even surpass human intelligence \cite{Macpherson2021}. This led, for example, to the development of artificial neural networks, a class of machine learning algorithms that underlie modern advances in the field of \gls{ai} \cite{Cox2014}. Other examples are so called \glspl{cnn}, which are used in computer vision and inspired by the architecture of the visual ventral stream or \glspl{rnn} mimicking the functioning of working memory \cite{Macpherson2021, Fukushima1982, Yin2020}. On the other side, \glspl{cnn} and \glspl{rnn} are used to understand functional and organizational properties of the visual system \cite{Yamins2014} or working memory \cite{Kim2021} which highlights the bilateral relationship between machine learning and neuroscience and so its use in neuroscience is widespread. The goals incorporate not only solving practical problems in biomedical engineering or data processing, but also gaining insights and drawing conclusions by testing hypotheses and developing new theories.\\
Traditionally, machine learning has been the core building block for the development of intelligent systems that can automate tasks or enhance and assist humans in performing their tasks. Examples of automation in the field of neuroscience can be found, among others, in relation to the processing of neuroscientific data addressing low reproducibility and subjectivity. \citeauthor{Jas2017} \cite{Jas2017} use unsupervised machine learning to automate data prepossessing, i.e. rejecting and interpolating bad data segments, in the analysis of data from \gls{eeg} or \gls{meg}. In addition, machine learning is increasingly used for electrical source imaging to solve the inverse problem, i.e. finding neural sources that give rise to the recorded \gls{eeg} activity \cite{Cui2019,Hecker2021}. Other applications involve classification to automatically segment images from \gls{mri}, e.g. in grey or white matter, or the automatic detection and quantification of cell activation in calcium imaging \cite{Akkus2017}. In medical settings furthermore machine learning can be applied with the goal to support clinical decision making, i.e. assist in diagnosis, risk assessment by predicting health status or forecasting of treatment responses \cite{Woo2017}. Examples include classification to support the diagnosis of Alzheimer's disease based on EEG \cite{Gallego-Jutgl√†2015} or \gls{mri} \cite{Vemuri2008} data as well as the automated identification of lesions in radiographic brain images \cite{Zhou208} or the detection of epileptic seizures based on EEG data \cite{Vandecasteele2020}. Beyond that, machine learning is often used in developing devices with the goal to assist, augment or enhance humans capabilities such as \glspl{bci}. In \glspl{bci}, neural activity is decoded, using classification to generate control commands for various external devices such as computers or prosthetic limbs \cite{Saha2021, Anumanchipalli2019}. In this context unsupervised methods, i.e. dimensionality reduction, often are used to deal with the high dimensionality and variability of measured neural activity \cite{Brunton2019}.\\
While the examples described so far have focused on solving specific problems, machine learning algorithms provide mechanistic insights into the information structure of data and the generalizability of hypotheses \cite{Hastie2009}. Especially in areas where high-dimensional data is prevalent, such as in neuroscience, the use of supervised as well as unsupervised machine learning offers insight by extracting complex patterns purely data driven \cite{Bzdok2017}. Dimensionality reduction, for example, makes it possible to describe the structure of high-dimensional data with fewer properties \cite{Cunningham2014}. Two common methods in the analysis of neuroscientific data are \gls{pca} or \gls{ica} which are used to uncover meaningful patterns in the data. Especially \gls{ica} is increasingly applied to uncover brain networks based on \gls{fmri} data \cite{Varoquaux2010} or to uncover the statistical sources in \gls{eeg} which can be used to infer components of noise but as well on brain processes during behavior \cite{Strophal2018}.  In addition, with \gls{dmd}, \citeauthor{Brunton2016} \cite{Brunton2016} apply a method to \gls{ecog} that allows to map both the spatial and temporal structure of sleep spindle networks as well as task related brain networks involved in movements. Besides dimensionality reduction, clustering is used to identify subgroups at different levels ranging from molecules to participant groups giving rise to new testable hypothesis \cite{Vu1601}. By applying a nonlinear dimensionality reduction technique called \gls{umap} and clustering to data of extracellular waveforms recorded in the premotor cortex of macaques, \citeauthor{Lee2021} \cite{Lee2021} were able to identify subgroups of neurons with previously unknown diversity in terms of dynamics and laminar distribution. Furthermore, in another study analysing \gls{fmri} during different cognitive tasks, \citeauthor{Hawco2021} \cite{Hawco2021} assessed the variability structure of brain activation patterns and identified a spectrum of participants brain activation patterns that coincides with task performance. These results were only possible using advanced methods from machine learning. Besides unsupervised approaches, supervised approaches are being used to map brain function to external variables, such as group membership, behavior or stimuli, with the ultimate goal to identify predictive variables or features \cite{Glaser2019}. Under this objective classification of group membership, inferences can be made about the underlying mechanisms of the phenomenon under study. This is often of interest to inform clinical theory in the development of novel biomarkers, for example in autism \cite{Deshpande2013} or Alzheimer's disease \cite{Farina2020}, as well as in basic neuroscience. In this context, regression is used to predict the neural response to an external variable, such as a particular stimulus, to create a so-called encoding model, which can be used to test and compare brain computational theories \cite{Kriegskorte2019, Naselaris2011}. \citeauthor{Benjamin2018} \cite{Benjamin2018}, for example, show the superiority of using modern machine learning in predicting spike rates based on behavioral measurements. Contrary, decoding models are created with the goal to predict external variables from neural activity providing insights in the information content of neural measurements. This type of analysis is often referred to as \gls{mvpa} because its goal is to detect multivariate patterns, e.g., a set of voxels in fMRI or an electrical pattern at a given time in EEG, associated with an experimental condition based on goodness of classification or regression \cite{Holdgraf2017}. For example, neural dynamics in visual object perception \cite{Cauchoix2014} or working memory \cite{Bae2018} have been studied using decoding approaches applied to \gls{eeg} recordings. Decoding was also linked to another variable, such as group membership \cite{Csizmadia2021, Bae2020} or symptom scores \cite{Coutanche2011} , to reveal interesting interrelationships with neural representations of the condition under study.\\
In summary, the application of machine learning in neuroscience is diverse. In addition to a close theoretical integration of the two fields, machine learning offers solutions to engineering problems in supporting and augmenting human performance as well as in data processing. In basic sciences, it is used to extract patterns from highly complex data and gain insights into the predictive power of variables. This can contribute to the verification and formation of new hypotheses.


\subsection{Methodological considerations}
Methods from supervised and unsupervised machine learning are possible candidates. In unsupervised machine learning, the goal is to find structure in the data. This includes methods for dimensionality reduction and clustering. Dimensionality reduction for example allows us to describe the structure of high dimensional data in fewer properties [20]. Two common methods in the analysis of neurophysiological data are the principal component analysis (PCA) and independent component analysis (ICA). These allow the detection of spatial patterns in the data that represent the underlying network characteristics of neurophysiological data [8]. In addition, with dynamic mode decomposition (DMD), Brunton, Johnson, et al. [21] apply for the first time a method to electrophysiological data that allows us to map both the spatial and temporal structure of the network structure of neurophysiological data.
In supervised machine learning, models are created that can predict a certain outcome based on input data. This method is used to detect neuronal representations of the environment or certain behaviors as well as group memberships and to identify relevant markers [22].
In the context of lifespan changes, complex brain network behavior based on EEG data could be extracted and visualized using dimensionality reduction. Supervised machine learning methods could be used to detect representations of the environment and behavior and to draw conclusions about the differentiation of brain networks. Automatic detection of group membership could further provide new predictors of nervous system states.

Besides solving computational problems algorithms from machine learning offer additional value for scientific inquiry including the possibility for the automatic analysis of complex multidimensional data \cite{Brunton2019,Breiman2001}. In this approach, rather than assuming that data is generated by an underlying stochastic model as in classical statistical modelling the mechanisms are treated as unknown which may overcome inaccuracies in the analysis \cite{Breiman2001}. Furthermore, by extracting generalizable principles from the complex interaction of features it offers additional values to traditional hypothesis-driven approaches \cite{Vu1601,Bzdok2017}.\\

% SUBSUBSECTION {ABGRENZUNBG ZU KLASSISCH STATISTISCHEN VERFAHREN????}

% \subsection{Age related reorganization of the brain}
% \label{subsec:Aging}
% In general aging is an ongoing process that can be detected at multiple interacting biological systems operating on several spatial and temporal scales contributing to the complexity of the phenomenon \cite{Mooney2016}. The most prominent consequences of this are declines in cognitive and sensorimotor abilities challenging the daily life of older adults.
% Age related reorganization processes are detectable at the whole body. This is underpinned by multiple interacting biological systems operating on several spatial and temporal scales contributing to the complexity of the phenomenon \cite{Mooney2016}. At the behavioral level these processes are noticeable in changes in cognitive, motor and sensory functioning [QUELLE]. 
% On a structural level aging has been associated with a reduction in gray matter with an onset early in life 

% \subsubsection{Machine Learning usages in aging neuroscience}
% TEXT