Machine Learning emerged in the 1950s to enable computers to learn without being explicitly programmed \cite{Samual1959}. It is defined by computational methods combining fundamental concepts from computer science, statistics, probability and optimization that automatically extract patterns and trends, i.e., \textit{learn} from data \cite{Hastie2009}. The notion of \textit{learning} therein describes the automated inference of general rules based on the observation of examples using algorithms with the goal to solve a certain task or problem \cite{Von_luxburg2011}. Often, in its basic form these tasks involve making predictions based on learned relationships or the extraction of information based on automatically detected patterns and structures from data. Many problems can be formulated by these tasks, and machine learning has found its way into everyday lives not only since the current hype about generative systems. Examples can be found in numerous areas, such as predicting stock prices, personalized advertising based on past search patterns, or autonomous driving \cite{Rudin2014}.\\
Also in science machine learning is increasingly used as a complementary method to classical statistical analysis because of the ability to make predictions as well as to deal with the multidimensional structure and non linearity in real world data sets for drawing inference \cite{Bzdok2018}. Especially in areas where high-dimensional data is prevalent, such as in neuroscience, the use of methods from machine learning offers insight by extracting complex patterns purely data driven \cite{Brunton2019}. In terms of \gls{eeg} this means that machine learning can help identify subtle patterns and nonlinear relationships from the multidimensional complex structure of \gls{eeg} data, allowing for more accurate and efficient analysis of brain recordings. A wide variety of methods are available for this purpose, which can be roughly characterized on the basis of various properties. 

\subsection{Forms of machine learning}
The three main forms of machine learning are supervised, unsupervised, and reinforcement learning which are defined by the type of feedback a machine learning algorithm has access to during learning \cite{Shalev2014}.\\
In supervised machine learning the goal is to learn a generalizable relationship between data and associated information, so-called labels or target. This can then be used to predict the label of new data that not have been used during the process of learning. If the labels are categorical, the prediction task is called classification; for continuous labels, the term is regression.\\
The goal of unsupervised machine learning is to find hidden structure in data without taking into account associated labels. This could be grouping similar data points, i.e. clustering, or uncovering a meaningful low dimensional representation of the data, i.e. dimensionality reduction. This type of learning is also referred to as \textit{knowledge discovery}\cite{Murphy2012}.\\
Reinforcement learning describes the task to learn optimal actions to solve a certain problem by maximizing the reward linked to that action.

\begin{figure*}[h]
  \dummyfig{Categories of ML} 
  \caption{Categories of ML}
  \label{fig1:ml_types}
\end{figure*}

In practice however a clear separation is often not possible. As such, dimensionality reduction can also be supervised, i.e. labels are provided to learn a new representation of the data \cite{mcinnes2018umap}. Besides, in semi-supervised learning, for example, the goal is the same as for supervised learning. However the data set used to learn the relationship contains both, labeled and unlabeled examples and the hope is to build a stronger representation by providing more information in form of data \cite{Burkov2019}. \\
In addition traditional machine learning is often contrasted to deep learning methods involving the use of artificial neural networks which are composed of many layers of interconnected nodes often used in an end to end fashion in which features are extracted within the network. Usually they require huge amount of data and computational power. In the context of this thesis the tasks considered involve the processing of a \gls{eeg} from experiments with mid to small sample sizes with the goal to learn meaningful patterns and relationships in data. The most commonly used forms in this contexts are supervised and unsupervised learning focusing traditional machine learning. For this reason these forms will be the focus in the following chapters.

\begin{tcolorbox}[breakable, enhanced]
    \subsection*{Excursus: How does a machine learn?}
    "A computer program is said to learn from experience $E$ with respect to some task $T$ and some performance measure $P$, if its performance on $T$, as measured by $P$, improves with experience $E$" \cite{Mitchell1997}. In other words learning in the context of machine learning typically involves using solving a specific task by using algorithms that improve their performance by using example data. There are numerous algorithms designed to solve the problems outlined above. Thereby some basic building blocks can be defined which can be used to describe computational learning in a formal way. In this thesis the view of statistical learning theory is considered, notation is adapted from \citeauthor{Shalev2014}\cite{Shalev2014}, \citeauthor{Von_luxburg2011}\cite{Von_luxburg2011}.\\
    Learning always is based on data, i.e. measurable information of some phenomenon, which consists of attributes of the phenomenon, so called features, and in supervised learning an associated label. It can mathematically defined as open bounded set $\mathcal{Z}\subset\mathbb{R}^n$ of dimension $n$. Typically there is only a set of examples or training data $S=\{z_i,...,z_m\}\subset{Z}^m$ available, where $i = 1,\dots,m$, and each $z_i$ is sampled independently from $\mathcal{Z}$ according to an underlying probability distribution $\mathcal{D}$. Thus the only assumption made is that the example data are independent and identically distributed. No assumption on $D$ is made.\\
    In supervised learning $\mathcal{Z}$ comprises the space of input data $\mathcal{X}$ and the space of labels or output $\mathcal{Y}$. The example data $S$ consists of labeled input-output pairs $z_i=x_i,y_i\in(\mathcal{X}\times\mathcal{Y})^m$, where $x_i$ is an input data vector and $y_i$ is its corresponding output label. The pairs are sampled by some unknown joint probability distribution $\mathcal{D}$ on the space $\mathcal{X}\times\mathcal{Y}$.\\
    The space $\mathcal{Z}$ in unsupervised learning comprises the input data space $\mathcal{X}$ only and the example set $S$ consists of unlabelled examples $z_i=x_i\in\mathcal{X}^m$, sampled according to some unknown probability distribution $\mathcal{D}$ on the space $\mathcal{X}$.\\
    \\
    Learning ultimately can be thought of as approximating an underlying ground truth function $f$, also called model, that represents the relationship between input and output in supervised learning, i.e., 
    \begin{equation}
    f:\mathcal{X}\rightarrow\mathcal{Y},
    \end{equation}
    or the mapping to a space of hidden patterns or structure $\mathcal{W}\subset\mathbb{R}^p$, where $p$ can be equal or smaller than $n$, i.e,
    \begin{equation}
    f:\mathcal{X}\rightarrow\mathcal{W}.
    \end{equation}
    The learning task can be conceptualized as searching through the space of all possible solution functions. As this is not feasible a finite class of functions or hypotheses is typically selected a priory. Thus, learning can also be thought of as selecting a hypothesis $h$ from a space of potential solutions $\mathcal{H}$, i.e. $\mathcal{H}=\{h:\mathcal{X}\rightarrow\mathcal{Y}\}$ in supervised learning and $\mathcal{H}=\{h:\mathcal{X}\rightarrow\mathcal{W}\}$ in unsupervised learning. \\
    A learner or learning algorithm is the means of selecting the best element from $\mathcal{H}$.
    The cost of a false prediction or an inaccurate representation of the data is quantified using a loss function, $\ell:\mathcal{H}\times\mathcal{Z}\rightarrow\mathbb{R}_+$. In other words it measures how well a specific hypothesis is doing.\\
    \\
    The expected risk, as measure of the average loss of a hypothesis, $h\in\mathcal{H}$ with respect to the probability distribution $\mathcal{D}$ over $\mathcal{Z}$ and can be defined as
    \begin{equation}
    L_{D}(h):=\mathbb{E}_{z\sim D}[\ell(h,z)]
    \end{equation}
    Theoretically a learner should select a hypothesis with lowest possible expected risk. 
    However the underlying probability distribution is unknown. Nevertheless by using $S$ the expected risk can be estimated by using the empirical risk over the training data. This is defined by:
    \begin{equation}
    L_{S}(h):=\frac{1}{m}\sum_{i=1}^m\ell(h,z_i).
    \end{equation}
    Following this, learning can be formalized solving an optimization problem of the form: 
    \begin{equation}
    \hat{h}=\arg\min_{h\in\mathcal{H}}L_{S}(h),
    \end{equation}
    which can be solved computationally. In parameterized models, this often involves the automated selection of those parameters $\theta\in\Theta$  of a chosen class of models that minimize $L_{S}(h_\theta)$. This optimization problem can then be solved by various methods such as gradient descent,  or e.g., analytically using least squares estimation. The solution $\hat{h}$ is the learned model that can be used to solve the task at hand, e.g. predicting the label of new input data or uncovering patterns or structure in data. This is known as \gls{erm}.\\
    Upon \gls{erm} more complex learning paradigms can be used addressing common problems such as overfitting, in which the learned hypothesis to closely relies on the training data and therefore has low generalization performance, e.g. regularized risk minimization which introduces regularization to \gls{erm} or structural risk minimization that penalizes complex models and encourages simplicity.\\
    \\
    Although most machine learning can conceptualized within the framework of \gls{erm} there are models that instead of minimizing risk assume that the underlying distribution over the data has a specific parametric form and the goal is to estimate these parameters by using \gls{mle} which seeks to find the model parameters that maximize the likelihood of the observed data under the assumed parametric distribution, i.e. \\
    \begin{equation}
    \hat{\theta}_{\text{MLE}} = \arg\max_{\theta\in\Theta}\prod_{i=1}^{m}p_{\theta}(z_i),
    \end{equation}
    where $p_{\theta}(z)$ is the joint probability function of the assumed parametric distribution and $\hat{\theta}_{\text{MLE}}$ is the estimated value of the parameter vector $\theta$.
\end{tcolorbox}

\subsection{Applications in the context of aging research}
Traditionally, machine learning has been the core building block for the development of intelligent systems that can automate tasks or enhance and assist humans in performing their tasks. In the medical field the hope is to develop intelligent medical systems to inform clinical theory and support clinical decision making, i.e. assist in diagnosis, and risk management by predicting health status or forecasting of treatment responses \cite{Woo2017}. In this context, supervised learning is often used to identify markers from \gls{eeg} by identifying signal features that are predictive of a particular disease or health condition, which is highly important in terms of promoting a healthy aging trajectory \cite{Babiloni_AlzCons2021,Mei2021}. An application is the estimation of biological age based on regression models trained on the basis on neural data, e.g. \gls{eeg}, recorded in large population studies \cite{Engemann2022}. Using data of an individual person, a regression model can predict the age of that person. If the brain appears older than it would chronologically, this can be an early indication of an unfavorable state of health \cite{Gonneaud2021}.\\
Another in the context of aging highly relevant application is the development of devices with the goal to assist, augment or enhance humans capabilities such as \glspl{bci}. In \glspl{bci}, neural activity is decoded, using classification to generate control commands for various external devices such as computers or prosthetic limbs \cite{Saha2021, Anumanchipalli2019}. Decoding hereby refers to learning a classification or regression model that is capable of predicting behavioral outcomes or cognitive states based on neural data, e.g. \gls{eeg}.\\
Beyond the application in \glspl{bci}, decoding techniques are widely used in neuroscientific research to gain insights into the neural mechanisms underlying perception, cognition, and behavior. This type of analysis is often referred to as \gls{mvpa} because its goal is to detect multivariate patterns, e.g., a set of voxels in \gls{fmri} or an electrical pattern at a given time in \gls{eeg}, associated with an experimental condition \cite{Holdgraf2017}.  While the use has a long history in the field of \gls{fmri} analysis, it has only become more widespread in the field of \gls{eeg} in recent years.  Therefore, decoding approaches to understand age-related reorganization are mostly limited to fMRI studies. A common approach is to quantify dedifferentiation, i.e. the loss of neural specificity, at the individual level. Since dedifferentiation by definition results in more similar brain activation patterns for different tasks or stimuli, poorer performance of classifiers trained to discriminate between them based on neural recordings is indicative of a less distinctive neural representation \cite{Koen2019, Park2010}. \\ 
Furthermore, the classification of group membership or group level regression can provide information about interesting relationships and their generalizability. Particularly for \gls{eeg} markers representing functional network characteristics can reveal insightful findings about the relationship to age-related changes \cite{Petti2016}.\\
In addition to the application of supervised learning algorithms, unsupervised learning algorithms have long been used in neuroscience. Unsupervised methods are often used as a preprocessing step to reduce the complexity of \gls{eeg} data or provide a framework for statistical source imaging but can also provide interesting insights into the structure of data sets. For example dimensionality reduction algorithms can provide insights into high-dimensional data by providing a possibility to visualize high dimensional patterns of \gls{eeg} data \cite{Kottlarz2020, Banville2021}. This can be used to study the temporal structure of \gls{eeg} signals in terms of the dynamic of brain networks with aging \cite{Brunton2016, vieluf2018age}.\\
In summary, the use of machine learning is very diverse and ranges from engineering applications to scientific knowledge discovery. Especially in the latter case, it offers the advantage of automated extraction of patterns from highly complex data that can contribute to the study of age-related changes. While decoding approaches are particularly interesting for tracking age-related changes in the organization of neural systems, such as the level of differentiation, group analysis could provide new insights into datasets. The combination with unsupervised learning algorithms could be particularly beneficial and used for the visualization of high-dimensional data.

\subsubsection{State of the art approaches}
As mentioned above, decoding approaches as well as dimensionality reduction methods are effective tools for investigating age-related changes of brain activity. Especially in decoding approaches, classification methods that allow to predict a certain experimental condition or a group membership are particularly suitable. The process of selecting a suitable learning algorithm, i.e. a classifier, involves an iterative approach \cite{Hastie2009}. The available data is usually divided into training and testing sets and various classifier are trained and validated within the training portion, which is called cross-validation. The best performing model is then finally tested on the testing portion to estimate its ability to generalize (see Figure \ref{fig1:cv_worklflow}). Different classifiers may have varying strengths and weaknesses, and some may be better suited to a specific task than others. Some popular examples include \glspl{svm}, \gls{lda}, and \glspl{rf} (see \cite{shoorangiz2021eeg} for an overview). \Glspl{svm} finds the optimal hyperplane in a high-dimensional space to separate the data points into different classes, \gls{lda} finds a linear combination of features that best separates the classes and \gls{rf} builds multiple decision trees and aggregates their prediction \cite{shoorangiz2021eeg}.\\
While novel applications highlight the use of deep neural networks capable of learning on the basis of raw data, i.e., end to end, their advantage comes into play with large labeled data resources, which are often expensive to acquire in the case of \gls{eeg} \cite{Banville2021}. Especially for comparatively smaller data sets, traditional learning approaches can be more efficient with good performance and promise better interpretability \cite{Gemein2020}.


\begin{figure*}[h]
  \dummyfig{Typical cross validation workflow } 
  \caption{State of the art approaches}
  \label{fig1:cv_worklflow}
\end{figure*}

% Depending on the learning objective or task, the procedure for the application of machine learning algorithms differs. Some aspects should be considered especially for \gls{eeg} data, since one typically deals with high-dimensional data, i.e. time series of multiple sensors with auto-correlative structure and a low signal to noise ratio to avoid overfitting of the machine learning model. In this case, the model overweights properties of the data used to learn, such as noise, and is thus not suitable to generalize to unknown data. While novel applications highlight the use of deep neural networks capable of learning on the basis of raw data, i.e., end to end, their advantage comes into play with large labeled data resources, which are often expensive to acquire in the case of \gls{eeg} \cite{Banville2021}. Especially for comparatively smaller data sets, traditional learning approaches can be more efficient with good performance and promise better interpretability \cite{Gemein2020}. 

% \subsubsection{Feature extraction and knowledge discovery}
% Due to the low signal to noise ratio and high complexity, \gls{eeg} data are typically represented by parameters, so called features, that are believed to be related to the problem being learned. Typical features include common \gls{eeg} parameters of time, frequency, time-frequency, connectivity and information theoretical parameters which are extracted for each sensor (see \cite{Gemein2020} for common choices). Even if this approach leads to generalizable models with large data sets and a large number of combined features, the chosen sensor-level features are often less flexible and generalizable due to low spatial resolution as well as vulnerable to low signal-to-noise ratios.\\
% To address the low spatial resolution, some approaches emphasize the computation of the source of the \gls{eeg} in the brain using biophysical models as a preprocessing step prior to feature extraction \cite{Khan2018, Westner2018}. In addition to biophysical source modeling, other approaches use supervised and unsupervised decomposition techniques that belong to the field of dimensionality reduction and thus can be used as a preprocessing step for further prediction tasks, e.g., classification or regression, or provide information themselves in the sense of knowledge discovery. These methods applied to \gls{eeg} signals aim at \textit{unmixing} the highly correlated sensor time series by assumptions about the underlying signal components. For example \Gls{ica} assumes statistical independence thus produce maximally statistical independent signal components. \Gls{pca}, on the other hand, assumes that the extracted components are maximally uncorrelated to each other extracting orthogonal components capturing the largest amount of variance in the data. One method that explicitly considers the temporal structure of the signals is \gls{dmd}, which requires the extracted signal patterns (modes) to be temporally coherent, thus accounting for the network character of the brain \cite{Brunton2016}. In addition, methods such as \gls{csp} \cite{Blankertz2008} or xDAWN \cite{rivet2009xdawn} have proven to be particularly useful preprocessing steps before prediction tasks. These supervised methods automatically extract signal components that correlate with the labels to be predicted.\\
% While supervised and unsupervised ways of examining the complex \gls{eeg} signals in terms of components and patterns to generate knowledge, non-linear methods such as \gls{tsne} and \gls{umap} take into account the non-linear relationships between the data points and provide a lower-dimensional representation of the data that is often easier to interpret and visualize \cite{mcinnes2018umap}. These methods can be particularly useful for exploring the relationships between different \gls{eeg} features or for identifying subgroups within a dataset.\\
% It is important to note that these methods can be applied not only to the \gls{eeg} signals itself, but also to previously extracted \gls{eeg} parameters or in combination in terms of knowledge discovery. Thus, the use of supervised and unsupervised dimension reduction provides data-driven insights into the complex underlying information, but also serves as preprocessing for further tasks such as prediction (see Figure \ref{fig1:ml_approoaches}.

% \begin{figure*}[h]
%   \dummyfig{State of the art approaches} 
%   \caption{State of the art approaches}
%   \label{fig1:ml_approoaches}
% \end{figure*}

% \subsubsection{Prediction}
% When the objective is to generate a model for label prediction, a range of linear and non-linear methods can be employed depending on the data characteristics and the goal of the prediction, such as classification or regression. Some popular examples of such models include \glspl{svm}, \gls{lda}, and \glspl{rf} (see \cite{shoorangiz2021eeg} for an overview). \Glspl{svm} finds the optimal hyperplane in a high-dimensional space to separate the data points into different classes, \gls{lda} finds a linear combination of features that best separates the classes and \gls{rf} builds multiple decision trees and aggregates their prediction \cite{shoorangiz2021eeg}.\\
% To approximate the performance of a predictive model a dataset is typically divided into a training and testing set. The training set is used for learning a model whereas the testing set is used to estimate the generalization performance to new unseen data, i.e. data which was not used during the process of training. The training data can further be divided into a training and validation portion in order to compare different model types or user defined settings of a learning algorithms, so called hyperparameters. However, this three time division may drastically reduce the data size usable for training and my result in flawed generalization evaluation due to the randomness of the split. Therefore several procedures can be applied. In a simple k-fold cross-validation, for example, the training data is divided k-times. Thus each time a different subset of the data is used for validation while the rest is used for training. Usually this is repeated for a range of models and subsequent hyperparamters and the model and hyperparameter performing best on average are selected for final testing. A more advanced method denoted nested cross-validation adds a second k-fold cross-validation loop for the final model evaluation (see Figure \ref{fig1:CV} for a visual representation of the procedures).    

% \begin{figure*}[h]
%   \dummyfig{Cross-validation procedures} 
%   \caption{Cross-validation procedures}
%   \label{fig1:CV}
% \end{figure*}

