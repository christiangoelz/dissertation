
\subsection{A definition}
Machine Learning emerged in the 1950s as a subbranch of \gls{ai} to enable computers to learn without being explicitly programmed \cite{Samual1959}. It is defined by computational methods combining fundamental concepts from computer science, statistics, probability and optimization that automatically extract patterns and trends, i.e., \textit{learn} from data \cite{Hastie2009}. The notion of \textit{learning} therein describes the automated inference of general rules based on the observation of examples using algorithms \cite{Von_luxburg2011}. These examples should represent machine-readable data, may contain few or multiple properties, so-called features, and may be multidimensional with variable sources for example sensor recordings or pixel values.\\
The three main forms of machine learning are supervised, unsupervised, and reinforcement learning which are defined by the type of feedback a machine learning algorithm has access to during learning. In application, however, a clear distinction is often difficult, so that mixed forms such as semi-supervised learning find application \cite{Russell2015}. 

\paragraph{Supervised learning}
In supervised machine learning the goal is to build a function or model that represents a generalizable relationship between input variables, known as features, and their corresponding output, referred to as labels. A learning algorithm is provided with labeled examples to optimize its performance and improve accuracy, i.e. it receives feedback on the correctness of a prediction it has made \cite{Russell2015}. The notion of generalizability is critical here and describes the ability of the learned model to predict the labels of new or \textit{unseen} data \cite{Daumé2017}. If the labels are categorical the learning task is called classification and if the labels are continuous the task at hand is regression. A standard example found in many text books is the task of email classification. Therein the input data might be emails associated with labels spam/not spam and the task is to produce a function that is able to automatically label new incoming emails \cite{Shalev2014}. For regression a standard example would be to predict the price of a house based on some features like the area or number of rooms \cite{Ng2022cs229notes}.

\paragraph{Unsupervised learning}
The goal of unsupervised learning is to find hidden structure or \textit{interesting patterns} in data without taking into account associated labels. Since there is no underlying truth defined, this type of learning is less well defined and is also referred to as \textit{knowledge discovery} \cite{Murphy2012}. Unsupervised machine learning includes many algorithms that can be further categorized according to their purpose, such as algorithms for clustering or dimensionality reduction. Clustering describes the automatically grouping of similar data points together in fewer subgroups or clusters. The goal of dimensionality reduction is to express the data in fewer properties without losing information characterizing the data \cite{Cunningham2014}. 

\paragraph{Reinforcement and other forms of learning}
Besides supervised and unsupervised, reinforcement learning is considered the third category of machine learning. The goal therein is to learn optimal actions to solve a certain problem by maximizing the reward linked to that action \cite{Murphy2012}.\\
There are also mixed forms of learning types. In Semi-supervised learning, for example, the goal is the same as for supervised learning. However the data set used to learn the model contains both, labeled and unlabeled examples and the hope is to build a stronger representation by providing more information in form of data \cite{Burkov2019}. 

\subsection{A mathematical framework of learning}
The process of learning can be formally defined using statistical learning theory \cite{Von_luxburg2011,Shalev2014}. In this mathematical framework a learning task, be it classification, regression, dimensionality reduction or clustering shares a couple of fundamental building blocks. \\
First there is an arbitrary set of examples, $\mathcal{Z}$, based on which learning can happen. . Then there is a space of potential solutions to the learning task, $\mathcal{H}$, also called hypothesis space. Last the \textit{cost} of a false prediction or an inaccurate representation of the data is quantified using a loss function, $\ell:\mathcal{H}\times\mathcal{Z}\rightarrow\mathbb{R}_+$, that guides the learning algorithm towards an optimal solution.\\

With that we can formally define supervised learning as: 

And unsupervised learning as: 


In a general learning framwork for suppervised learning we assume an underlying 

In unsupervised learning $\mathcal{Z}$ represents only a set of data $\mathcal{X}$, in supervised a set of associated labels $\mathcal{Y}$ is provided, i.e. $\mathcal{Z}=\mathcal{X}\times\mathcal{Y}$
Further, $\mathcal{D}$ represents a random probability distribution over $\mathcal{Z}$ from which a training data set $S={z_1,...,z_n}$ is drawn at random.

In supervised machine learning we assume 

In general, the choice of loss function depends on the specific problem at hand and the nature of the target variable. For example, in a regression problem where the target variable is a continuous variable, a common choice of loss function is the squared error loss, which measures the squared difference between the prediction and the true value of the target variable. On the other hand, in a classification problem where the target variable is a categorical variable, a common choice of loss function is the cross-entropy loss, which measures the difference between the predicted probability distribution and the true probability distribution of the target variable.\\




Thus, the goal of a learning algorithm can be defined as selecting $h \in \mathcal{H}$ that minimizing $\ell$. As the underlying distribution from which $D$ was drawn is unknown the empirical loss is typically defined as: 
\begin{equation}
EmpLoss_{L,D}(h)=\frac{1}{m}\sum_{(x,y)\in{D}}\elle(h,)).
\end{equation}
Following the best hypothesis $\hat{h}^*$ is chosen based on: 
\begin{equation}
\hat{h}^*=\operatorname{argmin}_{h\in{\mathcal{H}}}EmpLoss_{L,D}(h) .
\end{equation}



\subsection{Machine learning applied to electroencephalography}
BLABLA Was EEG Analysen mit ML denn generell bringen 

In the case of \gls{eeg} $\mathcal{X}$ is a bounded set in $\mathbb{R}^{n\times m}$ often represented as matrix $X$ containing $n$ sensors recordings of voltage fluctuations in $\mu$V across $m$ time points. 


\subsection{Applications}
Especially in areas where high-dimensional data is prevalent, such as in neuroscience, the use of supervised as well as unsupervised machine learning offers insight by extracting complex patterns purely data driven \cite{Bzdok2017}. With regard to the application on \gls{eeg} data it allows to address some of the main limitations of this method and automatically detect relevant patterns and their relation to variables of interest. Various research approaches use machine learning to extract information from the highly complex data and to relate it to experimental conditions or phenomena or to detect brain states.\\
\\
Traditionally, machine learning has been the core building block for the development of intelligent systems that can automate tasks or enhance and assist humans in performing their tasks. In the medical field the hope is to develop intelligent medical systems to inform clinical theory and support clinical decision making, i.e. assist in diagnosis, risk assessment by predicting health status or forecasting of treatment responses \cite{Woo2017}. In this context supervised learning is frequently use to identify biomarkers from \gls{eeg} by identifying signal characteristics that are predictive of a certain disease or health condition \cite{Babiloni_AlzCons2021,Mei2021}. An exciting application in the context of this work is the estimation of biological age based on regression models trained on the basis on neural data, e.g. \gls{eeg}, recorded in large population studies \cite{Engemann2022}. Using data of an individual person, a model can predict the age of the person. If the brain appears older than it would chronologically, this can be an early indication of an unfavorable state of health \cite{Gonneaud2021}. Another in the context of healthy aging highly relevant application is the development of devices with the goal to assist, augment or enhance humans capabilities such as \glspl{bci}. In \glspl{bci}, neural activity is decoded, using classification to generate control commands for various external devices such as computers or prosthetic limbs \cite{Saha2021, Anumanchipalli2019}.\\
Decoding hereby refers to learning a classification or regression model that is capable of predicting behavioral outcomes or cognitive states characteristics. Beyond the application in \glspl{bci}, decoding techniques are widely used in neuroscientific research to gain insights into the neural mechanisms underlying perception, cognition, and behavior. This type of analysis is often referred to as \gls{mvpa} because its goal is to detect multivariate patterns, e.g., a set of voxels in \gls{fmri} or an electrical pattern at a given time in \gls{eeg}, associated with an experimental condition based on goodness of classification or regression \cite{Holdgraf2017}. This can also be applied to study the reorganization of the brain of aging individuals. \Citeauthor{Carb2011}\cite{Carb2011} for example applied machine learning to \gls{fmri} to decode brain activation during a motor task and detected age-related changes such as dedifferentiation of the motor system. Furthermore, the classification of group membership or group level regression can provide information about interesting relationships and their generalizability. Particularly for \gls{eeg} markers representing functional network characteristics, this can reveal insightful findings about the relationship to age-related changes \cite{Petti2016}.\\
Unsupervised methods are often used as a preprocessing step to reduce the complexity of the underlying data or provide a framework for statistical source imaging, but can also provide interesting insights into the structure of data sets. Clustering is used in aging research to identify clinical subgroups or phenotype of healthy aging \cite{Marron2019}. Furthermore, dimensionality reduction algorithms can provide insights into high-dimensional data and, for example, into the temporal structure of \gls{eeg} signals of Alzheimer's patients \cite{Smailovic2019}.\\
In summary, the use of machine learning is very diverse and ranges from engineering applications to scientific knowledge discovery. Especially in the latter case, it offers the advantage of automated extraction of patterns from highly complex data that can contribute to the study of age-related changes. 
Allerdings ist es nicht einfach geeignete Methoden auszuwählen, da diese von den zur Verfügung stehenden Daten sowie der Zielstellung der 

\subsection{A general machine learning pipeline}



% space of examples, $\mathcal{Z}$, the space of potential solutions, $\mathcal{H}$,  also called hypothesis space and a \textit{loss function}, $\ell$, which measures the "cost" of a false prediction or an inaccurate representation of the data in supervised and unsupervised learning respectively. \\
% In classification for example 


% \Citeauthor{Russell2015} \cite{Russell2015} define the process and with that the basic formal principle of supervised machine learning algorithms as follows. Learning typically is based on a training set $D=\{(x_i, y_i)\}_{i=1}^{m}$ containing $m$ examples $(x_i , y_i)$ in which $x_i$ is a feature vector and $y_i$ the associated label for the \({i}^{th}\) example. It is assumed that $y_i$ was generated by some unknown function $y=f(x)$ and the goal of learning is to discover a function $h$ from a given hypothesis space $\mathcal{H}$ that approximates the true function $f$. Note that $f$ does not necessarily represent a function but can also be stochastic in nature, so that the goal is to learn a probability distribution, i.e. $P(Y,x)$. In this process a loss function $L(y,\hat{y})$ is defined, which determines the error between the actual label, $y$ and the models prediction $\hat{y}$. Thus, the goal of a learning algorithm can be defined as minimizing $L$. As the underlying distribution from which $D$ was drawn is unknown the empirical loss is typically defined as: 
% \begin{equation}
% EmpLoss_{L,D}(h)=\frac{1}{m}\sum_{(x,y)\in{D}}L(y,h(x)) .
% \end{equation}
% Following the best hypothesis $\hat{h}^*$ is chosen based on: 
% \begin{equation}
% \hat{h}^*=\operatorname{argmin}_{h\in{\mathcal{H}}}EmpLoss_{L,D}(h) .
% \end{equation}
% In practice, this often involves the automated selection of those parameters $\theta$ of a chosen class of models ($\mathcal{H}$) that minimize $EmpLoss$. This optimization problem can then be solved by various algorithms such as gradient descent, stochastic gradient descent and sequential minimal optimization. The choice depends on the problem to be solved as well as the selected model. 


  

% \begin{figure*}[h]
%   \dummyfig{Categories of ML} 
%   \caption{Categories of ML}
%   \label{fig1:ml_types}
% \end{figure*}

% % \subsection{A formal description of learning}
% % A formal definition of how a machine learns, can be made based on statistical learning theory. \citeauthor{Von_luxburg2011}\cite{Von_luxburg2011} describe this framework for supervised learning as follows. Given a learning problem, \(\mathcal{X}\) denotes the space of inputs and \(\mathcal{Y}\) the space outputs. The goal of learning is then to learn a functional relationship $f:\mathcal{X}\rightarrow\mathcal{Y}$. 
% % Further, $\mathcal{P}$ represents a random probability distribution over $\mathcal{Z}=\mathcal{X}\times\mathcal{Y}$ from which a training data set $S$ is drawn at random. Using $S$ the goal of supervised machine learning is to learn a function $h:\mathcal{X}\rightarrow\mathcal{Y}$. The performance of this function can be measured using a so called loss function $ell$ measuring the error a prediction 
% % The learning process typically involves adjusting the parameters defining the model by presenting the input-output pairings in the training data set to minimize the error, called loss, between the actual value and the models prediction. This is done by optimizing a cost function, defined as the summarized loss over the training examples. Often used optimization algorithms are gradient descent, stochastic gradient descent and sequential minimal optimization\footnote{Since the focus of this work is rather on the application and the evaluation with regard to scientific knowledge gain, a detailed mathematical introduction of the different algorithms is omitted at this point (see \cite{Ng2022cs229notes} for a detailed description).} and the choice depends on the problem to be solved as well as the selected model. It should be noted that not all learning algorithms are based on minimizing a defined cost function, but that there are also algorithms that implicitly optimize a certain criterion, such as Decision Trees or the K-Neirest Neighbor algorithm.\\
% % More formally, supervised learning is to learn a function, also called hypothesis, mapping the space of input values \(\mathcal{X}\) to the space of output values \(\mathcal{Y}\), i.e. \(h_{\theta}:\mathcal{X}\rightarrow\mathcal{Y}\). by minimizing a cost function \(J(\theta)\). 
% % Here \(\theta\) are the parameters, or weights, defining the space of functions and the cost function can be defined as 
% % $$J(\theta) = \frac{1}{m} \sum_{i=1}^{m} L(h_{\theta}(x_i),y_i)$$.  
% % Here \(L\) is called loss function which quantifies the error between true label \(y_i\) and predicted label 

% % A standard example found in many text books is the task of email classification. Therein the input data might be emails associated with labels spam/not spam and the task is to produce a function that is able to automatically label new incoming emails \cite{Shalev2014}. For regression a standard example would be to predict the price of a house based on some features like area, number of rooms, etc. \cite{Ng2022cs229notes}.
% % Inductive reasoning is central to this process and involves using labelled input-output pairs known as training data set $S=\{(x_i, y_i)\}_{i=1}^{m}$, in which $x_i \in \mathbb R^{n}$ is a ${n}$-dimensional feature vector and $y_i$ the associated label for the \({i}^{th}\) example generated by some unknown function $y_i=f(x_i)$ \cite{Russell2015, Saha2021}. An algorithm is then used to induce a function $h$ approximating the true function $f$ \cite{Daumé2017}. Note that $f$ does not necessarily represent a function but can also be stochastic in nature, so that the goal is to learn a probability distribution, i.e. $P(Y,x)$ \cite{Russell2015}. 

% \subsection{Methodological considerations}
% Methods from supervised and unsupervised machine learning are possible candidates. In unsupervised machine learning, the goal is to find structure in the data. This includes methods for dimensionality reduction and clustering. Dimensionality reduction for example allows us to describe the structure of high dimensional data in fewer properties [20]. Two common methods in the analysis of neurophysiological data are the principal component analysis (PCA) and independent component analysis (ICA). These allow the detection of spatial patterns in the data that represent the underlying network characteristics of neurophysiological data [8]. In addition, with dynamic mode decomposition (DMD), Brunton, Johnson, et al. [21] apply for the first time a method to electrophysiological data that allows us to map both the spatial and temporal structure of the network structure of neurophysiological data.
% In supervised machine learning, models are created that can predict a certain outcome based on input data. This method is used to detect neuronal representations of the environment or certain behaviors as well as group memberships and to identify relevant markers [22].
% In the context of lifespan changes, complex brain network behavior based on EEG data could be extracted and visualized using dimensionality reduction. Supervised machine learning methods could be used to detect representations of the environment and behavior and to draw conclusions about the differentiation of brain networks. Automatic detection of group membership could further provide new predictors of nervous system states.

